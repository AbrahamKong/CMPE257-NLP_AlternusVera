{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Merging_models_final.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "pMUKSO14po93"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbrahamKong/CMPE257-NLP_AlternusVera/blob/master/Merging_models_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Streamlite App of all models**"
      ],
      "metadata": {
        "id": "UtVPmP2epKX_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3i9hxcLZvU9n"
      },
      "outputs": [],
      "source": [
        "! pip install streamlit\n",
        "! pip install transformers\n",
        "! pip install spacy --upgrade\n",
        "! python -m spacy download en_core_web_md\n",
        "! python -m spacy download en_core_web_sm\n",
        "! pip install swifter\n",
        "! pip install spacy-universal-sentence-encoder\n",
        "!pip install dgl==0.6.0\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove*.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! streamlit run final_app.py & npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "N2CmGK1Zv6Jl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **All Model**"
      ],
      "metadata": {
        "id": "AdWbr1zte2HR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile final_app.py\n",
        "import streamlit as st\n",
        "import requests\n",
        "import json\n",
        "import csv\n",
        "import gensim\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.pipeline import Pipeline\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "import seaborn as sns\n",
        "import re\n",
        "import nltk\n",
        "import string\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import sparse\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from numba import jit\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import dgl\n",
        "from dgl.dataloading import GraphDataLoader\n",
        "from dgl.data import DGLDataset\n",
        "from dgl.nn.pytorch import GraphConv, GATConv, GatedGraphConv, DotGatConv\n",
        "from dgl.nn import AvgPooling, MaxPooling\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn import CrossEntropyLoss\n",
        "import torch.nn as nn\n",
        "import operator\n",
        "import scipy.sparse as sp\n",
        "import spacy\n",
        "import joblib\n",
        "import torch.nn.functional as F\n",
        "import spacy_universal_sentence_encoder\n",
        "from transformers import pipeline\n",
        "\n",
        "\n",
        "avinash_model = joblib.load(\"/content/liar_final.sav\")\n",
        "# abraham_model = joblib.load(\"/content/abraham_final.sav\")\n",
        "\n",
        "\n",
        "medium_nlp = spacy_universal_sentence_encoder.load_model('en_use_md')\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def load_models(model, folds, avinash_model):\n",
        "    model_list = []\n",
        "    \n",
        "    for i in range(folds):\n",
        "        # print('Loading weights')\n",
        "        model.load_state_dict(torch.load(f'/content/drive/My Drive/ML-Spring-2022/Data Miners/Assignments/Pooja/Assignment - 12/Graph_model/fold-{i}.pt'))\n",
        "        model.eval()\n",
        "        model_list.append(model)\n",
        "        model_list.append(avinash_model)\n",
        "        #model_list.append(abraham_model)        \n",
        "    return model_list\n",
        "\n",
        "def cognitive_complexisty(text):\n",
        "  words=[x for x in text.split(' ')]\n",
        "  char_count_per_word= [len(x) for x in text.split(' ')]\n",
        "  Average_character_per_word=sum(char_count_per_word)/len(char_count_per_word)\n",
        "  unique_word_ratio = len(set(words))/len(words)\n",
        "  return (Average_character_per_word,unique_word_ratio)\n",
        "\n",
        "def text_cleaning(text):\n",
        "    '''\n",
        "    Make text lowercase, remove text in square brackets,remove links,remove special characters\n",
        "    and remove words containing numbers.\n",
        "    '''\n",
        "    text = text.lower()\n",
        "    text = re.sub('\\[.*?\\]', '', text)\n",
        "    text = re.sub(\"\\\\W\",\" \",text) # remove special chars\n",
        "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
        "    text = re.sub('<.*?>+', '', text)\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
        "    text = re.sub('\\n', '', text)\n",
        "    text = re.sub('\\w*\\d\\w*', '', text)\n",
        "    \n",
        "    return text\n",
        "\n",
        "def consolidated_micro_factors(dataframe):\n",
        "  dataframe['cleaned_text']=dataframe['news'].apply(text_cleaning)\n",
        "  dataframe.drop(columns=['news'],inplace=True)\n",
        "  dataframe['cognitive_complexisty']=dataframe['cleaned_text'].apply(cognitive_complexisty)\n",
        "  Average_character_per_word,unique_word_ratio = zip(*dataframe['cognitive_complexisty'].values)\n",
        "  dataframe['Average_character_per_word']=Average_character_per_word\n",
        "  dataframe['unique_word_ratio'] = unique_word_ratio\n",
        "  dataframe.drop(columns=['cognitive_complexisty'],inplace=True)\n",
        "  dataframe['Verbal_immediacy']=dataframe['cleaned_text'].apply(findPronoun_Verb)\n",
        "  ratio_of_past_tense_verb, ratio_of_PRON_first_person,ratio_of_PRON_third_person = zip(*dataframe['Verbal_immediacy'].values)\n",
        "  dataframe['ratio_of_past_tense_verb'] = ratio_of_past_tense_verb\n",
        "  dataframe['ratio_of_PRON_first_person'] = ratio_of_PRON_first_person\n",
        "  dataframe['ratio_of_PRON_third_person'] = ratio_of_PRON_third_person\n",
        "  dataframe.drop(columns=['Verbal_immediacy'],inplace=True)\n",
        "  @jit\n",
        "  def test(x):\n",
        "    return classifier1(x)\n",
        "\n",
        "  #emotion1 = dataframe['cleaned_text'].swifter.apply(test)\n",
        "  emotion1 = dataframe['cleaned_text'].apply(test)\n",
        "  \n",
        "  anger=[]\n",
        "  disgust=[]\n",
        "  fear=[]\n",
        "  joy=[]\n",
        "  neutral=[]\n",
        "  sadness=[]\n",
        "  surprise=[]\n",
        "  \n",
        "  for x in range(len(emotion1)):\n",
        "    # print(emotion1[x][0][0]['score'])\n",
        "    anger.append(emotion1[x][0][0]['score'])\n",
        "    disgust.append(emotion1[x][0][1]['score'])\n",
        "    fear.append(emotion1[x][0][2]['score'])\n",
        "    joy.append(emotion1[x][0][3]['score'])\n",
        "    neutral.append(emotion1[x][0][4]['score'])\n",
        "    sadness.append(emotion1[x][0][5]['score'])\n",
        "    surprise.append(emotion1[x][0][6]['score'])\n",
        "  emotions_scores1_df = pd.DataFrame(data=[anger,disgust,fear,joy,neutral,sadness,surprise]).T.rename(columns={0:'anger',1:'disgust',2:'fear',3:'joy',4:'neutral',5:'sadness',6:'surprise'})\n",
        "  dataframe = pd.merge(dataframe,emotions_scores1_df,left_index=True,right_index=True)\n",
        " # dataframe['word_vectors']=dataframe['cleaned_text'].swifter.apply(lambda x : list(medium_nlp(x).vector))\n",
        "  dataframe['word_vectors']=dataframe['cleaned_text'].apply(lambda x : list(medium_nlp(x).vector))\n",
        "  dataframe.drop(columns=['cleaned_text'],inplace=True)\n",
        "  X=dataframe.iloc[:,1:-1]\n",
        "  X_word_vector = pd.DataFrame(list(dataframe['word_vectors']))\n",
        "  dataframe_final = pd.merge(X,X_word_vector,left_index=True,right_index=True)\n",
        "  loaded_model = joblib.load(\"/content/deception_final.sav\")\n",
        "  loaded_model1 = joblib.load(\"/content/fakedit_final.sav\")\n",
        " # final_model= joblib.load(\"/content/liar_final.sav\")\n",
        "  deception=loaded_model.predict(dataframe_final)\n",
        "  dataframe_final['deception']=deception\n",
        "  fakedit_label=loaded_model1.predict(dataframe_final)\n",
        "  dataframe_final['fakedit_label']=np.argmax(fakedit_label,axis=1)\n",
        "  #prediction =  np.argmax(final_model.predict(dataframe_final),axis=1)\n",
        "  return dataframe_final\n",
        "\n",
        "def findPronoun_Verb(text):\n",
        "    PRON_first_person = 0\n",
        "    PRON_third_person = 0\n",
        "    pron_count=0\n",
        "    past_tens_verb_count=0\n",
        "    verb_count=0\n",
        "    doc = nlp(text)\n",
        "    for token in doc:\n",
        "          if token.pos_=='PRON':\n",
        "            pron_count+=1\n",
        "            if token.morph.get(\"Person\") == [\"1\"] and token.morph.get(\"Number\") == ['Sing']:\n",
        "              PRON_first_person+=1\n",
        "            if token.morph.get(\"Number\")==[\"3\"]:\n",
        "              PRON_third_person+=1\n",
        "          if token.pos_=='VERB':\n",
        "            verb_count+=1\n",
        "            if token.morph.get(\"Tense\")==['Past']:\n",
        "              past_tens_verb_count+=1\n",
        "\n",
        "    ratio_of_past_tense_verb = past_tens_verb_count/verb_count if verb_count else 0\n",
        "    ratio_of_PRON_first_person = PRON_first_person/pron_count if pron_count else 0\n",
        "    ratio_of_PRON_third_person = PRON_third_person/pron_count if pron_count else 0\n",
        "    return ratio_of_PRON_first_person,ratio_of_PRON_third_person,ratio_of_past_tense_verb\n",
        "\n",
        "def get_final_AModel(dataframe_final):\n",
        "  final_model= joblib.load(\"/content/liar_final.sav\")\n",
        "  #prediction =  np.argmax(final_model.predict(dataframe_final),axis=1)\n",
        "  logits =  final_model.predict(dataframe_final)\n",
        "  return logits\n",
        "\n",
        "classifier1 = pipeline(\"text-classification\", model=\"j-hartmann/emotion-english-distilroberta-base\", return_all_scores=True, truncation='only_first')\n",
        "\n",
        "class args:\n",
        "    max_epochs = 10\n",
        "    lr = 1e-2\n",
        "    batch_size = 32\n",
        "    embedding_dim = 50\n",
        "    hidden_dim = 50\n",
        "    num_heads = 8   # used for attention model\n",
        "    n_folds = 5\n",
        "    window_size = 3\n",
        "\n",
        "\n",
        "def test(args, n_classes, test_data):\n",
        "  MAX_TRUNC_LEN = 128\n",
        "\n",
        "  word_embeddings_dim = args.embedding_dim\n",
        "\n",
        "  shuffle_doc_words_list = list(test_data['news'].values)\n",
        "\n",
        "  word_set = set()\n",
        "\n",
        "  # Returns list of tuples. The first element of each tuple specifies the word present in the description but not in the embeddings  and the second element specifies the count of that word in the descriptions. The tuples are sorted in the descending order of their count.\n",
        "  def check_coverage(vocab,embeddings_index):\n",
        "    a = {}\n",
        "    oov = {}\n",
        "    k = 0\n",
        "    i = 0\n",
        "    for word in tqdm(vocab):\n",
        "        try:\n",
        "            a[word] = embeddings_index[word]\n",
        "            k += vocab[word]\n",
        "        except:\n",
        "\n",
        "            oov[word] = vocab[word]\n",
        "            i += vocab[word]\n",
        "            pass\n",
        "\n",
        "    # print('Found embeddings for {:.2%} of vocab'.format(len(a) / len(vocab)))\n",
        "    # print('Found embeddings for  {:.2%} of all text'.format(k / (k + i)))\n",
        "    sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n",
        "\n",
        "    return sorted_x\n",
        "\n",
        "  # Returns dictionary with keys as words in the sentences and values as their count of occurence\n",
        "  def build_vocab(sentences, verbose =  True):\n",
        "    vocab = {}\n",
        "    for sentence in tqdm(sentences, disable = (not verbose)):\n",
        "        for word in sentence:\n",
        "            try:\n",
        "                vocab[word] += 1\n",
        "            except KeyError:\n",
        "                vocab[word] = 1\n",
        "    return vocab\n",
        "\n",
        "  # Returns list of adjacency matrix and list of node matrix\n",
        "  def build_graph(test_data, start, end, truncate = False, weighted_graph = True):\n",
        "    x_adj = []\n",
        "    x_feature = []\n",
        "    doc_len_list = []\n",
        "    vocab_set = set()\n",
        "    # print(\"vocab\", vocab)\n",
        "\n",
        "    for i in tqdm(range(start, end)):\n",
        "\n",
        "        doc_words = list(test_data['news'].values)[i].split()\n",
        "        if truncate:\n",
        "            doc_words = doc_words[:128]\n",
        "        doc_len = len(doc_words)\n",
        "\n",
        "        doc_vocab = list(set(doc_words))\n",
        "        doc_nodes = len(doc_vocab)\n",
        "\n",
        "        doc_len_list.append(doc_nodes)\n",
        "        vocab_set.update(doc_vocab)\n",
        "\n",
        "        doc_word_id_map = {}\n",
        "        for j in range(doc_nodes):\n",
        "            doc_word_id_map[doc_vocab[j]] = j\n",
        "\n",
        "        # sliding windows\n",
        "        windows = []\n",
        "        if doc_len <= 3:\n",
        "            windows.append(doc_words)\n",
        "        else:\n",
        "            for j in range(doc_len - 3 + 1):\n",
        "                window = doc_words[j: j + 3]\n",
        "                windows.append(window)\n",
        "\n",
        "        word_pair_count = {}\n",
        "        for window in windows:\n",
        "            for p in range(1, len(window)):\n",
        "                for q in range(0, p):\n",
        "                    word_p = window[p]\n",
        "                    word_p_id = word_id_map[word_p]\n",
        "                    word_q = window[q]\n",
        "                    word_q_id = word_id_map[word_q]\n",
        "                    if word_p_id == word_q_id:\n",
        "                        continue\n",
        "                    word_pair_key = (word_p_id, word_q_id)\n",
        "                    # word co-occurrences as weights\n",
        "                    if word_pair_key in word_pair_count:\n",
        "                        word_pair_count[word_pair_key] += 1.\n",
        "                    else:\n",
        "                        word_pair_count[word_pair_key] = 1.\n",
        "                    # bi-direction\n",
        "                    word_pair_key = (word_q_id, word_p_id)\n",
        "                    if word_pair_key in word_pair_count:\n",
        "                        word_pair_count[word_pair_key] += 1.\n",
        "                    else:\n",
        "                        word_pair_count[word_pair_key] = 1.\n",
        "    \n",
        "        row = []\n",
        "        col = []\n",
        "        weight = []\n",
        "        features = []\n",
        "        for key in word_pair_count:\n",
        "            p = key[0]\n",
        "            q = key[1]\n",
        "            row.append(doc_word_id_map[vocab[p]])\n",
        "            col.append(doc_word_id_map[vocab[q]])\n",
        "            weight.append(word_pair_count[key] if weighted_graph else 1.)\n",
        "        adj = sp.csr_matrix((weight, (row, col)), shape=(doc_nodes, doc_nodes))\n",
        "    \n",
        "        for k, v in sorted(doc_word_id_map.items(), key=lambda x: x[1]):\n",
        "            features.append(word_embeddings[k] if k in word_embeddings else oov[k])\n",
        "\n",
        "        x_adj.append(adj)\n",
        "        x_feature.append(features)\n",
        "\n",
        "    \n",
        "    return x_adj, x_feature\n",
        "\n",
        "  # Get graph and app target label\n",
        "  class GraphDataset(DGLDataset):\n",
        "    def __init__(self, x_adj, x_feature, targets = None):\n",
        "        self.adj_matrix = x_adj\n",
        "        self.node_matrix = x_feature\n",
        "        self.targets = targets\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.adj_matrix)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        scipy_adj = self.adj_matrix[idx]\n",
        "        G = dgl.from_scipy(scipy_adj)\n",
        "        G.ndata['feat'] = torch.stack([torch.tensor(x, dtype = torch.float) for x in self.node_matrix[idx]])\n",
        "        \n",
        "        \n",
        "        if self.targets is not None:\n",
        "            label = self.targets[idx]\n",
        "            \n",
        "            return G, torch.tensor(label, dtype = torch.long)\n",
        "        \n",
        "        return G\n",
        "\n",
        "  # Graph Neural Network with Attention Layers where a dot product is performed between node features  \n",
        "  class GATDotClassifier(nn.Module):\n",
        "    def __init__(self, in_dim, hidden_dim, num_heads, n_classes):\n",
        "        super(GATDotClassifier, self).__init__()\n",
        "        self.hid_dim = hidden_dim\n",
        "        self.gat1 = DotGatConv(in_dim, hidden_dim, num_heads, allow_zero_in_degree=True)\n",
        "        self.gat2 = DotGatConv(hidden_dim*num_heads, hidden_dim, 1, allow_zero_in_degree=True)\n",
        "        self.avgpooling = AvgPooling()\n",
        "        self.classify = nn.Linear(hidden_dim, n_classes)\n",
        "\n",
        "    def forward(self, g, h):\n",
        "        # Apply graph convolution and activation.\n",
        "        bs = h.shape[0]\n",
        "        h = F.relu(self.gat1(g, h))\n",
        "        h = h.reshape(bs, -1)\n",
        "        h = F.relu(self.gat2(g, h))\n",
        "        h = h.reshape(bs, -1)\n",
        "        h = self.avgpooling(g, h)\n",
        "        \n",
        "        return self.classify(h)\n",
        "\n",
        "  for doc_words in shuffle_doc_words_list:\n",
        "      words = doc_words.split()\n",
        "      word_set.update(words)\n",
        "      \n",
        "  vocab = list(word_set)\n",
        "  vocab_size = len(vocab)\n",
        "\n",
        "  word_id_map = {}\n",
        "  for i in range(vocab_size):\n",
        "      word_id_map[vocab[i]] = i\n",
        "      \n",
        "  oov = {}\n",
        "  for v in vocab:\n",
        "      oov[v] = np.random.uniform(-0.1, 0.1, word_embeddings_dim)\n",
        "      \n",
        "  GLOVE_EMBEDDING_PATH = '/content/glove.6B.50d.txt'\n",
        "\n",
        "  word_embeddings = {}\n",
        "\n",
        "  with open(GLOVE_EMBEDDING_PATH, 'r') as f:\n",
        "    for line in f.readlines():\n",
        "      data = line.split()\n",
        "      word_embeddings[str(data[0])] = list(map(float,data[1:]))\n",
        "\n",
        "  vocab = build_vocab(list(test_data['news'].apply(lambda x:x.split())))\n",
        "\n",
        "  oov = check_coverage(vocab,word_embeddings)\n",
        "  oov[:10]\n",
        "  window_size = 3\n",
        "  #labels = test_data['label'].unique()\n",
        "  #label2idx = {l:i for i,l in enumerate(sorted(labels))}\n",
        "  #idx2label = {v:k for k,v in label2idx.items()}\n",
        "\n",
        "  # test_data['label'] = test_data['label'].apply(lambda x: label2idx[x])\n",
        "  num_classes = n_classes\n",
        "  window_size = 3\n",
        "  x_adj, x_feature = build_graph(test_data, start=0, end=len(test_data), weighted_graph = True)\n",
        "  \n",
        "  testdataset = GraphDataset(x_adj, x_feature)\n",
        "  testloader = GraphDataLoader(testdataset, batch_size = args.batch_size, shuffle = False)\n",
        "  \n",
        "  model = GATDotClassifier(args.embedding_dim, args.hidden_dim, args.num_heads, num_classes)\n",
        "  model_list = load_models(model, args.n_folds, avinash_model)\n",
        "  \n",
        "  pred_list = []\n",
        "  pred_softmax = []\n",
        "\n",
        "  # print(vocab)\n",
        "\n",
        "  with torch.no_grad():\n",
        "      for idx, G in enumerate(tqdm(testloader)):\n",
        "          h = G.ndata['feat'].float()\n",
        "          logits = 0\n",
        "          for mod in model_list:\n",
        "            if mod == avinash_model:\n",
        "              intent = test_data.rename(columns = {'news':'statement'})\n",
        "              final_dataset = consolidated_micro_factors(intent.copy())\n",
        "              logits =  get_final_AModel(final_dataset)\n",
        "            else:\n",
        "              log = mod(G, h)\n",
        "              # blending of logits from all 5 models. This helps in getting more robust predictions. \n",
        "              logits_numpy = (log.softmax(-1) / (args.n_folds)).numpy()\n",
        "              #logits += log.softmax(-1) / (args.n_folds)\n",
        "\n",
        "      logits_avg = np.mean((logits, logits_numpy), axis = 0)\n",
        "      # print(logits_avg)\n",
        "\n",
        "      result = logits_avg.argmax(axis = 1)\n",
        "  return result\n",
        "\n",
        "def fetch_news():\n",
        "  url = ('https://newsapi.org/v2/top-headlines?'\n",
        "       'country=us&'\n",
        "       'apiKey=78f8b5ac018e4c568badb6877ac4f778')\n",
        "  response = requests.get(url)\n",
        "  news_data = response.json()\n",
        "  json_object = json.dumps(news_data)\n",
        "  with open(\"news.json\", \"w\") as outfile:\n",
        "    outfile.write(json_object) \n",
        "  df = pd.read_json('/content/news.json')\n",
        "  news_data = pd.json_normalize(news_data,record_path=['articles']).iloc[:10,:]\n",
        "  return news_data\n",
        "\n",
        "st.header(\"Click the below button to extract 10 news from Newsapi\")\n",
        "if st.button('Fetch News'):\n",
        "  news = fetch_news()\n",
        "  st.dataframe(news)\n",
        "  st.markdown(\"Perform Prediction for each news\")\n",
        "  for index, row1 in news.iterrows():\n",
        "    st.header(f\"News - {index}\")\n",
        "    st.subheader('title')\n",
        "    st.write(row1['title'])\n",
        "    st.subheader(\"News Description\")\n",
        "    st.write(row1['description'])\n",
        "    st.subheader(\"News Content\")\n",
        "    st.write(row1['content'])\n",
        "    st.subheader(\"Source\")\n",
        "    st.write(row1['source.name'])\n",
        "    st.subheader(\"Prediction\")\n",
        "    if row1['content'] ==None:\n",
        "      if row1['description']==None:\n",
        "        prediction = test(args, 6, pd.DataFrame({'news': row1['title']},index=[0]))\n",
        "      else:\n",
        "        prediction = test(args, 6, pd.DataFrame({'news': row1['description']},index=[0])) \n",
        "    else:\n",
        "      prediction = test(args, 6, pd.DataFrame({'news': row1['content']},index=[0]))\n",
        "\n",
        "    labels_dict = {'true' : 0, 'mostly-true': 1, 'half-true': 2, 'barely-true': 3, 'false': 4, 'pants-fire': 5}\n",
        "    finding_category={y: x for x, y in labels_dict.items()}\n",
        "    if prediction == 0:\n",
        "      st.write('true')\n",
        "    elif prediction ==1:\n",
        "      st.write('mostly-true')\n",
        "    elif prediction ==2:\n",
        "      st.write('half-true')\n",
        "    elif prediction ==3:\n",
        "      st.write('barely-true')\n",
        "    elif prediction ==4:\n",
        "      st.write('false')\n",
        "    else:\n",
        "      st.write('pants-fire')    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBxpNoOexFjG",
        "outputId": "67a75ee0-3d89-407c-da5c-e894d3076f21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting final_app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! streamlit run final_app.py & npx localtunnel --port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axNnkj-ibPEL",
        "outputId": "d2282949-a8b4-4dc8-8440-50641dcb74c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-05-13 18:11:32.882 INFO    numexpr.utils: NumExpr defaulting to 2 threads.\n",
            "\u001b[K\u001b[?25hnpx: installed 22 in 2.849s\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.2:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://35.199.60.83:8501\u001b[0m\n",
            "\u001b[0m\n",
            "your url is: https://great-books-warn-35-199-60-83.loca.lt\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Abraham_Model**"
      ],
      "metadata": {
        "id": "pMUKSO14po93"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "politifact_df = pd.read_csv('/content/politifact_abraham.csv')"
      ],
      "metadata": {
        "id": "VTUviU-91DwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "politifact_df.head(10)"
      ],
      "metadata": {
        "id": "eRIIqNqX7UWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cleaning(raw_news):\n",
        "    # import nltk\n",
        "    # nltk.download('punkt')\n",
        "    # nltk.download('wordnet')\n",
        "    \n",
        "    # 1. Remove non-letters/Special Characters and Punctuations\n",
        "    news = re.sub(\"[^a-zA-Z]\", \" \", raw_news)\n",
        "    \n",
        "    # 2. Convert to lower case.\n",
        "    news =  news.lower()\n",
        "    \n",
        "    # 3. Tokenize.\n",
        "    news_words = nltk.word_tokenize( news)\n",
        "    \n",
        "    # 4. Convert the stopwords list to \"set\" data type.\n",
        "    stops = set(nltk.corpus.stopwords.words(\"english\"))\n",
        "    \n",
        "    # 5. Remove stop words. \n",
        "    words = [w for w in  news_words  if not w in stops]\n",
        "    \n",
        "    # 6. Lemmentize \n",
        "    wordnet_lem = [ WordNetLemmatizer().lemmatize(w) for w in words ]\n",
        "    \n",
        "    # 7. Stemming\n",
        "    stems = [nltk.stem.SnowballStemmer('english').stem(w) for w in wordnet_lem ]\n",
        "    \n",
        "    # 8. Join the stemmed words back into one string separated by space, and return the result.\n",
        "    return \" \".join(stems)"
      ],
      "metadata": {
        "id": "twlqWyZmzHIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reference https://gist.github.com/AhmedHani/81bd40ff3ce4d9131f02c89fe099d490\n",
        "def get_sentences(text):\n",
        "    return re.split('\\? |, |!|\\n', text)\n",
        "\n",
        "def get_words(sentences):\n",
        "    return sum([sentence.split() for sentence in sentences], [])\n",
        "\n",
        "def get_average_word_length(words):\n",
        "    return sum([len(word) for word in words]) // len(words)\n",
        "  \n",
        "def get_average_word_based_sentence_length(sentences):\n",
        "    return sum([len(sentence.split()) for sentence in sentences]) // len(sentences)\n",
        "\n",
        "def get_average_chars_based_sentence_length(sentences):\n",
        "    return sum([len(sentence) for sentence in sentences]) // len(sentences)\n",
        "  \n",
        "def get_punctuations_count(text):\n",
        "    return len([c for c in text if c in string.punctuation])\n",
        "\n",
        "def vocab_richness_ratio(words):\n",
        "  return float(len(set(words))) / float(len(words))\n",
        "\n",
        "def shannon_entropy(words):\n",
        "  import math\n",
        "  import numpy as np\n",
        "  import scipy as sc\n",
        "  from collections import Counter\n",
        "\n",
        "  length = len(words)\n",
        "  freqs = Counter(words)\n",
        "  distribution = np.array(list(freqs.values()))\n",
        "  distribution = np.true_divide(distribution, length)\n",
        "    \n",
        "  E = sc.stats.entropy(distribution, base=2)\n",
        "\n",
        "  return E\n",
        "\n",
        "def syllables_count(words, vowels=\"aeiouy\"):\n",
        "  total_count = 0.0\n",
        "\n",
        "  for word in words:\n",
        "    word = word.lower()\n",
        "    count = 0\n",
        "  \n",
        "    if word[0] in vowels:\n",
        "      count += 1\n",
        "  \n",
        "    for i in range(1, len(word)):\n",
        "      if word[i] in vowels and word[i - 1] not in vowels:\n",
        "        count += 1\n",
        "\n",
        "        if word.endswith(\"e\"):\n",
        "          count -= 1\n",
        "    \n",
        "    if count == 0:\n",
        "      count = 1\n",
        "    \n",
        "    total_count += count\n",
        "\n",
        "  return total_count, (total_count // len(words)) # Average\n",
        "\n",
        "def flesch_reading_ease(words, sentences, avg_word_based_sentence_length):\n",
        "  # 206.835 - (1.015 * Average Number of Words Per Sentence) - (84.6 * Average Syllables Count Per Sentence)\n",
        "  _, avg_syllables_count = syllables_count(words)\n",
        "\n",
        "  return 206.835 - (1.015 * avg_word_based_sentence_length) - (84.6 * avg_syllables_count)"
      ],
      "metadata": {
        "id": "0qhyiM2pRkjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mergeLabel(r):\n",
        "        '''\n",
        "        Merge the PolitiFacts's 8 labels to Liar Liar's 6 data label\n",
        "        Change full-flop to pants-fire, and half-flip to barely-true\n",
        "        '''\n",
        "\n",
        "        # v = r['Label']\n",
        "\n",
        "        if (r == 'half-flip'):\n",
        "            r = 'pants-fire'\n",
        "        if (r == 'full-flop'):\n",
        "            r = 'barely-true'\n",
        "        return r \n"
      ],
      "metadata": {
        "id": "2woda8uuPEa-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mergeNumberLabel(r):\n",
        "        '''\n",
        "        Convert labels to Integers\n",
        "        '''\n",
        "\n",
        "        # v = r['Label']\n",
        "\n",
        "        if (r == 'true'):\n",
        "            r = 0\n",
        "        if (r == 'mostly-true'):\n",
        "            r = 1\n",
        "        if (r == 'half-true'):\n",
        "            r = 2\n",
        "        if (r == 'barely-true'):\n",
        "            r = 3\n",
        "        if (r == 'false'):\n",
        "            r = 4\n",
        "        if (r == 'pants-fire'):\n",
        "            r = 5\n",
        "        if (r == 'half-flip'):\n",
        "            r = 5\n",
        "        if (r == 'full-flop'):\n",
        "            r = 3\n",
        "        return r "
      ],
      "metadata": {
        "id": "_fxtoLSVY_T0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "politifact_df['Label'] = politifact_df[\"Label\"].apply(mergeNumberLabel)"
      ],
      "metadata": {
        "id": "V8r1TYKu0INV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "politifact_df.head(10)"
      ],
      "metadata": {
        "id": "by6EOqpa7kel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def corpursStyleEval(df): \n",
        "  train_final = df.copy()\n",
        "\n",
        "  train_final['label'] = train_final[\"Label\"].apply(mergeNumberLabel)\n",
        "\n",
        "  train_final['sentences'] = train_final['clean'].apply(get_sentences)\n",
        "  # train_final['n_sentences'] = politifact_df[\"sentences\"].apply(len)\n",
        "  # print(f'number of sentences: {n_sentences}')\n",
        "          \n",
        "  train_final['words'] = train_final['sentences'].apply(get_words)\n",
        "  # train_final['n_words'] = len(train_final['words'])\n",
        "  # print(f'number of words: {n_words}')\n",
        "          \n",
        "  train_final['avg_word_length'] = train_final['words'].apply(get_average_word_length)\n",
        "  # print(f'average word length: {avg_word_length}')\n",
        "          \n",
        "  train_final['avg_word_based_sentence_length'] = train_final['sentences'].apply(get_average_word_based_sentence_length)\n",
        "  # print(f'average word-based sentence length: {avg_word_based_sentence_length}')\n",
        "          \n",
        "  train_final['avg_char_based_sentence_length'] = train_final['sentences'].apply(get_average_chars_based_sentence_length)\n",
        "  # print(f'average char-based sentence length: {avg_char_based_sentence_length}')\n",
        "          \n",
        "  train_final['punc_count'] = train_final['clean'].apply(get_punctuations_count)\n",
        "  # print(f'punctuations count: {punc_count}')\n",
        "\n",
        "  train_final['richness_ratio'] = train_final['words'].apply(vocab_richness_ratio)\n",
        "\n",
        "  train_final['flesch_score']=0\n",
        "  for i in range(train_final.shape[0]):\n",
        "    train_final['flesch_score'][i] = flesch_reading_ease(train_final['words'][i], train_final['sentences'][i], train_final['avg_word_based_sentence_length'][i])\n",
        "\n",
        "  return train_final[['clean', 'label', 'richness_ratio', 'flesch_score']].copy()"
      ],
      "metadata": {
        "id": "CFrtEhSHpsgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "politifact_df['clean'] = politifact_df[\"Statement\"].apply(cleaning)\n",
        "politifact_df = corpursStyleEval(politifact_df)"
      ],
      "metadata": {
        "id": "4ABfjsWm1eid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import pickle\n",
        "\n",
        "from tensorflow.keras.layers import *\n",
        "from keras.layers import *\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n",
        "from tensorflow.keras import activations\n",
        "from keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "ZQ2TRM8M5LOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "politifact_df.to_csv('/content/final_abraham.csv')"
      ],
      "metadata": {
        "id": "Mz4FXX5A6vUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "politifact_df.head(10)"
      ],
      "metadata": {
        "id": "VM5GM26L7ASq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_final = politifact_df[politifact_df['flesch_score'].notna()]\n",
        "X = train_final[['richness_ratio', 'flesch_score']].copy()\n",
        "y = train_final['label']\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "MMWsrzaG5lk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "opt = Adam(learning_rate=0.01)\n",
        "def BiLstm():\n",
        "  model.add(layers.Dense(2, activation=\"relu\"))\n",
        "  model.add(layers.Dense(3, activation=\"relu\"))\n",
        "  model.add(Dense(6, activation=activations.softmax))\n",
        "  model.build(input_shape = X_train.shape)\n",
        "  model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "  return model"
      ],
      "metadata": {
        "id": "ks85Vxh54j2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "model = BiLstm()\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "mwuJs1WL4nYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train, y_train,\n",
        "                    validation_data=(X_test, y_test), \n",
        "                    epochs=200, \n",
        "                    batch_size=16, \n",
        "                   # callbacks=lrate, \n",
        "                    verbose=2)"
      ],
      "metadata": {
        "id": "3Ri6XdNm4x9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hmN_elC741z_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
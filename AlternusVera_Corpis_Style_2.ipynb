{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AlternusVera-Corpis_Style-2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPOcbzwB95oDg9wTkAEaKgr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbrahamKong/CMPE257-NLP_AlternusVera/blob/master/AlternusVera_Corpis_Style_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Alternus Vera "
      ],
      "metadata": {
        "id": "VYEr9nYELRPd"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhkYu-eMaHsJ"
      },
      "source": [
        "-----\n",
        "\n",
        "GitHub URL: \n",
        "\n",
        "\n",
        "### Liar Liar Pants on Fire Dataset Description \n",
        "- It has 3 files test, training and valid.\n",
        "- Each file has 14 columns\n",
        "    \n",
        "    Column 1: the ID of the statement ([ID].json).\n",
        "    \n",
        "    Column 2: the label.\n",
        "    \n",
        "    Column 3: the statement.\n",
        "    \n",
        "    Column 4: the subject(s).\n",
        "    \n",
        "    Column 5: the speaker.\n",
        "    \n",
        "    Column 6: the speaker's job title.\n",
        "    \n",
        "    Column 7: the state info.\n",
        "    \n",
        "    Column 8: the party affiliation.\n",
        "    \n",
        "    Column 9-13: the total credit history count, including the current statement.\n",
        "    \n",
        "    Column 14: the context (venue / location of the speech or statement).\n",
        "\n",
        "### Process \n",
        "- Load the Data\n",
        "- Distillation Process\n",
        "    - Data Cleaning and Text Preprocessing\n",
        "    - Visualization\n",
        "- **Feature 1 :** Corpus Style \n",
        "    \n",
        "\n",
        "### Team Contributions example:\n",
        "\n",
        "|Features  |  Member |\n",
        "|-----|-----|\n",
        "| Feature name(s)                         |  Member name(s) |  \n",
        "| Feature name(s)                 |  Member name(s) | \n",
        "| Feature name(s)                   |  Member name(s)  |   \n",
        "| Feature name(s)                             |  Member name(s) |\n",
        "\n",
        "\n",
        "\n",
        "#### What did I try and What worked? \n",
        "\n",
        "> Explain your work ...\n",
        "\n",
        "#### What did not work?\n",
        "\n",
        "> Explain your work ...\n",
        "\n",
        "\n",
        "#### What alternatives did you try?\n",
        "\n",
        "> Explain your work \n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Y65WRi_lLfXc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Pre-process"
      ],
      "metadata": {
        "id": "xK9ZHdpULV3C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mount Google Drive"
      ],
      "metadata": {
        "id": "1i3G1xOeLdMI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KuR8CcbHgjcF",
        "outputId": "2d441ba3-6431-4771-8807-e8441859a4ee"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path_data = \"/content/drive/MyDrive/SJSU/CMPE 257: Machine Learning/NLP/AlternusVera/input_data\""
      ],
      "metadata": {
        "id": "laVWj19lvHId"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "your_team=\"Data Miners/NLP-Alternus Vera/input_data\"\n",
        "# file_path_data =\"/content/drive/MyDrive/ML-Spring-2022/\"+your_team\n",
        "import os\n",
        "os.chdir(file_path_data)"
      ],
      "metadata": {
        "id": "zd-CTywSLRf5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIpAVBIRaHsM"
      },
      "source": [
        "### Reading the Data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install selenium\n",
        "# !pip install newspaper3k\n",
        "# ! pip install beautifulsoup4\n",
        "# ! pip install requests"
      ],
      "metadata": {
        "id": "7sSwN21hDRkM"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpWu_E9haHsN",
        "outputId": "8a8556cc-0a1c-4a4e-9cf9-9b155131b6ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "import gensim\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.pipeline import Pipeline\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import sparse\n",
        "# Code source: https://degravek.github.io/project-pages/project1/2017/04/28/New-Notebook/\n",
        "# Dataset from Chakraborty et al. (https://github.com/bhargaviparanjape/clickbait/tree/master/dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJWZA7MmaHsO",
        "outputId": "d0b0079b-da0f-4eb5-c29e-4d76564f2333"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2882: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
            "\n",
            "\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        }
      ],
      "source": [
        "# Read the test, training and valid data from files\n",
        "# Header = 0 indicates that the first line of the file contains column names,\n",
        "# As there is no Header, create a column names for each column in the dataset\n",
        "# delimiter = \\t indicates that the fields are seperated by tabs, and \n",
        "\n",
        "\n",
        "# test_filename = 'input_data/dataset/test.tsv'\n",
        "# train_filename = 'input_data/dataset/train.tsv'\n",
        "# valid_filename = 'input_data/dataset/valid.tsv'\n",
        "\n",
        "test_filename = file_path_data + '/test.tsv'\n",
        "train_filename = file_path_data + '/train.tsv'\n",
        "valid_filename = file_path_data + '/valid.tsv'\n",
        "\n",
        "colnames = ['jsonid', 'label', 'headline_text', 'subject', 'speaker', 'speakerjobtitle', 'stateinfo','partyaffiliation', 'barelytruecounts', 'falsecounts','halftruecounts','mostlytrueocunts','pantsonfirecounts','context']\n",
        "\n",
        "train_news = pd.read_csv(train_filename, sep='\\t', names = colnames, error_bad_lines=False)\n",
        "test_news = pd.read_csv(test_filename, sep='\\t', names = colnames, error_bad_lines=False)\n",
        "valid_news = pd.read_csv(valid_filename, sep='\\t', names = colnames, error_bad_lines=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### tweets from Donald Trump"
      ],
      "metadata": {
        "id": "zRLKYWoT1yvK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "id": "Af09YudpaHsO",
        "outputId": "27196823-d831-4b0e-ad90-44f81de709ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train dim: (10240, 14) test dim: (1267, 14)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       jsonid      label                                      headline_text  \\\n",
              "0   2635.json      false  Says the Annies List political group supports ...   \n",
              "1  10540.json  half-true  When did the decline of coal start? It started...   \n",
              "\n",
              "                              subject         speaker       speakerjobtitle  \\\n",
              "0                            abortion    dwayne-bohac  State representative   \n",
              "1  energy,history,job-accomplishments  scott-surovell        State delegate   \n",
              "\n",
              "  stateinfo partyaffiliation  barelytruecounts  falsecounts  halftruecounts  \\\n",
              "0     Texas       republican               0.0          1.0             0.0   \n",
              "1  Virginia         democrat               0.0          0.0             1.0   \n",
              "\n",
              "   mostlytrueocunts  pantsonfirecounts          context  \n",
              "0               0.0                0.0         a mailer  \n",
              "1               1.0                0.0  a floor speech.  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-67f5b830-0bcb-4070-9147-e148f429d7c2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>jsonid</th>\n",
              "      <th>label</th>\n",
              "      <th>headline_text</th>\n",
              "      <th>subject</th>\n",
              "      <th>speaker</th>\n",
              "      <th>speakerjobtitle</th>\n",
              "      <th>stateinfo</th>\n",
              "      <th>partyaffiliation</th>\n",
              "      <th>barelytruecounts</th>\n",
              "      <th>falsecounts</th>\n",
              "      <th>halftruecounts</th>\n",
              "      <th>mostlytrueocunts</th>\n",
              "      <th>pantsonfirecounts</th>\n",
              "      <th>context</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2635.json</td>\n",
              "      <td>false</td>\n",
              "      <td>Says the Annies List political group supports ...</td>\n",
              "      <td>abortion</td>\n",
              "      <td>dwayne-bohac</td>\n",
              "      <td>State representative</td>\n",
              "      <td>Texas</td>\n",
              "      <td>republican</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>a mailer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10540.json</td>\n",
              "      <td>half-true</td>\n",
              "      <td>When did the decline of coal start? It started...</td>\n",
              "      <td>energy,history,job-accomplishments</td>\n",
              "      <td>scott-surovell</td>\n",
              "      <td>State delegate</td>\n",
              "      <td>Virginia</td>\n",
              "      <td>democrat</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>a floor speech.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-67f5b830-0bcb-4070-9147-e148f429d7c2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-67f5b830-0bcb-4070-9147-e148f429d7c2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-67f5b830-0bcb-4070-9147-e148f429d7c2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# Display check the dimensions and the first 2 rows of the file.\n",
        "\n",
        "print('train dim:',train_news.shape, 'test dim:', test_news.shape)\n",
        "train_news.iloc[0:2]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_filename = file_path_data + '/trump_tweets.csv'\n",
        "\n",
        "trump_tweets = pd.read_csv(tweets_filename)"
      ],
      "metadata": {
        "id": "-1U9dS4u1vip"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trump_tweets = trump_tweets.drop(columns=['id', 'isRetweet', 'isDeleted', 'device', 'favorites', 'retweets', 'isFlagged'])"
      ],
      "metadata": {
        "id": "AZa0gqld15ue"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trump_tweets = trump_tweets[trump_tweets[\"text\"].str.startswith(\"http\")==False]\n",
        "# trump_tweets = trump_tweets[trump_tweets[\"text\"].str.startswith(\"RT\")==False]"
      ],
      "metadata": {
        "id": "i1t4yMCB2eLr"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trump_tweets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "jaJa-r-M2tCz",
        "outputId": "9041b26a-fe7f-483f-9be7-0ba2de18d1ac"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                    text                 date\n",
              "0      Republicans and Democrats have both created ou...  2011-08-02 18:07:48\n",
              "1      I was thrilled to be back in the Great city of...  2020-03-03 01:34:50\n",
              "2      RT @CBS_Herridge: READ: Letter to surveillance...  2020-01-17 03:22:47\n",
              "3      The Unsolicited Mail In Ballot Scam is a major...  2020-09-12 20:10:58\n",
              "4      RT @MZHemingway: Very friendly telling of even...  2020-01-17 13:13:59\n",
              "...                                                  ...                  ...\n",
              "56566  RT @RandPaul: I don’t know why @JoeBiden think...  2020-10-23 03:46:25\n",
              "56567  RT @EliseStefanik: President @realDonaldTrump ...  2020-10-23 03:42:05\n",
              "56568  RT @TeamTrump: LIVE: Presidential Debate #Deba...  2020-10-23 01:03:58\n",
              "56569  Just signed an order to support the workers of...  2020-10-22 21:04:21\n",
              "56570  Suburban women want Safety &amp; Security. Joe...  2020-10-22 18:31:46\n",
              "\n",
              "[55265 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-efd60acd-4528-4c64-a56e-918f3087ac85\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Republicans and Democrats have both created ou...</td>\n",
              "      <td>2011-08-02 18:07:48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I was thrilled to be back in the Great city of...</td>\n",
              "      <td>2020-03-03 01:34:50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>RT @CBS_Herridge: READ: Letter to surveillance...</td>\n",
              "      <td>2020-01-17 03:22:47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The Unsolicited Mail In Ballot Scam is a major...</td>\n",
              "      <td>2020-09-12 20:10:58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>RT @MZHemingway: Very friendly telling of even...</td>\n",
              "      <td>2020-01-17 13:13:59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56566</th>\n",
              "      <td>RT @RandPaul: I don’t know why @JoeBiden think...</td>\n",
              "      <td>2020-10-23 03:46:25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56567</th>\n",
              "      <td>RT @EliseStefanik: President @realDonaldTrump ...</td>\n",
              "      <td>2020-10-23 03:42:05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56568</th>\n",
              "      <td>RT @TeamTrump: LIVE: Presidential Debate #Deba...</td>\n",
              "      <td>2020-10-23 01:03:58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56569</th>\n",
              "      <td>Just signed an order to support the workers of...</td>\n",
              "      <td>2020-10-22 21:04:21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56570</th>\n",
              "      <td>Suburban women want Safety &amp;amp; Security. Joe...</td>\n",
              "      <td>2020-10-22 18:31:46</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>55265 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-efd60acd-4528-4c64-a56e-918f3087ac85')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-efd60acd-4528-4c64-a56e-918f3087ac85 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-efd60acd-4528-4c64-a56e-918f3087ac85');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "510WhC2yaHsP"
      },
      "source": [
        "## Data Cleaning and Text Preprocessing \n",
        "\n",
        "*Steps included in the preprocessing:*\n",
        "- Remove Special Characters and Punctuations\n",
        "- Lower case the news\n",
        "- Tokenization\n",
        "- Remove Stop Words\n",
        "- Lemmatization\n",
        "- Stemming \n",
        "- Spell Check "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKYeW-VJaHsP"
      },
      "source": [
        "###  Putting It All Together \n",
        "\n",
        "To make the code reusable, we need to create a function that can be called many times."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "WVbe8c7YaHsP"
      },
      "outputs": [],
      "source": [
        "def cleaning(raw_news):\n",
        "    # import nltk\n",
        "    # nltk.download('punkt')\n",
        "    # nltk.download('wordnet')\n",
        "    \n",
        "    # 1. Remove non-letters/Special Characters and Punctuations\n",
        "    news = re.sub(\"[^a-zA-Z]\", \" \", raw_news)\n",
        "    \n",
        "    # 2. Convert to lower case.\n",
        "    news =  news.lower()\n",
        "    \n",
        "    # 3. Tokenize.\n",
        "    news_words = nltk.word_tokenize( news)\n",
        "    \n",
        "    # 4. Convert the stopwords list to \"set\" data type.\n",
        "    stops = set(nltk.corpus.stopwords.words(\"english\"))\n",
        "    \n",
        "    # 5. Remove stop words. \n",
        "    words = [w for w in  news_words  if not w in stops]\n",
        "    \n",
        "    # 6. Lemmentize \n",
        "    wordnet_lem = [ WordNetLemmatizer().lemmatize(w) for w in words ]\n",
        "    \n",
        "    # 7. Stemming\n",
        "    stems = [nltk.stem.SnowballStemmer('english').stem(w) for w in wordnet_lem ]\n",
        "    \n",
        "    # 8. Join the stemmed words back into one string separated by space, and return the result.\n",
        "    return \" \".join(stems)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-eoSnAkjaHsQ",
        "outputId": "efeccf44-2632-4d7a-8682-08d6f64b6c89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "\n",
            "Time to clean, tokenize and stem train data: \n",
            " 10240 news: 0.13447338740030926 min\n",
            "\n",
            "\n",
            "Time to clean, tokenize and stem test data: \n",
            " 1267 news: 0.01324311097462972 min\n",
            "\n",
            "\n",
            "Time to clean, tokenize and stem valid data: \n",
            " 1284 news: 0.01279436747233073 min\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "# clean training and test data \n",
        "# create new column \"tokenized\"\n",
        "t1 = time.time()\n",
        "\n",
        "# Add the processed data to the original data. \n",
        "# Perhaps using apply function would be more elegant and concise than using for loop\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "train_news['clean'] = train_news[\"headline_text\"].apply(cleaning) \n",
        "\n",
        "t2 = time.time()\n",
        "print(\"\\nTime to clean, tokenize and stem train data: \\n\", len(train_news), \"news:\", (t2-t1)/60, \"min\")\n",
        "\n",
        "t1 = time.time()\n",
        "test_news['clean'] = test_news[\"headline_text\"].apply(cleaning)\n",
        "\n",
        "t2 = time.time()\n",
        "print(\"\\n\\nTime to clean, tokenize and stem test data: \\n\", len(test_news), \"news:\", (t2-t1)/60, \"min\")\n",
        "\n",
        "t1 = time.time()\n",
        "valid_news['clean'] = valid_news[\"headline_text\"].apply(cleaning)\n",
        "\n",
        "t2 = time.time()\n",
        "print(\"\\n\\nTime to clean, tokenize and stem valid data: \\n\", len(valid_news), \"news:\", (t2-t1)/60, \"min\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iK4nsnhtaHsQ"
      },
      "source": [
        "### [Google News corpus word2vec](http://mccormickml.com/2016/04/12/googles-pretrained-word2vec-model-in-python/)\n",
        "\n",
        "### Spell Check \n",
        "\n",
        "-  You can download the pre-trained model [**here**](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit)\n",
        "\n",
        "- Or clone it from GitHub [**GoogleNews-vectors-negative300**](https://github.com/mmihaltz/word2vec-GoogleNews-vectors)\n",
        "\n",
        "> It’s 1.5GB! It includes word vectors for a vocabulary of 3 million words and phrases that they trained on roughly 100 billion words from a Google News dataset. The vector length is 300 features.\n",
        "\n",
        "**3 million words * 300 features * 4bytes/feature = ~3.35GB**\n",
        "\n",
        "> This file consist of the word2vec -  pre-trained Google News corpus (3 billion running words) to word vector model (3 million 300-dimension English word vectors).\n",
        "\n",
        "> Look at the [**vocabulory list**](https://github.com/chrisjmccormick/inspect_word2vec/tree/master/vocabulary) used to train this model. Each text file contains 100,000 entries from the model. \n",
        "\n",
        "\n",
        ">  There are few things that this dataset contains and not. It has stop words like  “the”, “also”, “should” and does not have stop words like “a”, “and”, “of”. As I have removed the stop words the complexity is reduced as there is no need to check the spelling for stop words. \n",
        "\n",
        "> It does have numbers but in the form of entried wiht #. e.g., you won’t find “100”. But it does include entries like “###MHz_DDR2_SDRAM”. \n",
        "\n",
        "The model used [**WinPython-64bit-2.7.10.3**](https://winpython.github.io/) for efficient python distribution on Windows system. Helps to run the scripts in batches. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BT1bnST3aHsR"
      },
      "outputs": [],
      "source": [
        "# model = gensim.models.KeyedVectors.load_word2vec_format('input_data/GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
        "model = gensim.models.KeyedVectors.load_word2vec_format((file_path_data + '/GoogleNews-vectors-negative300.bin.gz'), binary=True)\n",
        "words = model.index2word\n",
        "\n",
        "w_rank = {}\n",
        "for i,word in enumerate(words):\n",
        "    w_rank[word] = i\n",
        "\n",
        "WORDS = w_rank"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DJ5YXYrwaHsR"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "def words(text): return re.findall(r'\\w+', text.lower())\n",
        "\n",
        "def P(word, N=sum(WORDS.values())): \n",
        "    \"Probability of `word`.\"\n",
        "    return - WORDS.get(word, 0)\n",
        "\n",
        "def correction(word): \n",
        "    \"Most probable spelling correction for word.\"\n",
        "    return max(candidates(word), key=P)\n",
        "\n",
        "def candidates(word): \n",
        "    \"Generate possible spelling corrections for word.\"\n",
        "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
        "\n",
        "def known(words): \n",
        "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
        "    return set(w for w in words if w in WORDS)\n",
        "\n",
        "def edits1(word):\n",
        "    \"All edits that are one edit away from `word`.\"\n",
        "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
        "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
        "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
        "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
        "    return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "def edits2(word): \n",
        "    \"All edits that are two edits away from `word`.\"\n",
        "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tBpLmy-_aHsR"
      },
      "outputs": [],
      "source": [
        "def spell_checker(text):\n",
        "    all_words = re.findall(r'\\w+', text.lower()) # split sentence to words\n",
        "    spell_checked_text  = []\n",
        "    for i in range(len(all_words)):\n",
        "        spell_checked_text.append(correction(all_words[i]))\n",
        "    return ' '.join(spell_checked_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AUbXpO18aHsR"
      },
      "outputs": [],
      "source": [
        "print(\"Before: \\n\", train_news['clean'][0] )\n",
        "t1 = time.time()\n",
        "train_news['clean'] = train_news['clean'].apply(spell_checker)\n",
        "t2 = time.time()\n",
        "print(\"\\nTime to spell check the train data: \\n\", len(train_news), \"news:\", (t2-t1)/60, \"min\")\n",
        "\n",
        "print(\"\\nAfter: \\n\",train_news['clean'][0] )\n",
        "train_news.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4zsTGRRKaHsS"
      },
      "outputs": [],
      "source": [
        "t1 = time.time()\n",
        "test_news['clean'] = test_news['clean'].apply(spell_checker)\n",
        "test_news.head(5)\n",
        "t2 = time.time()\n",
        "print(\"\\nTime to spell check the test data: \\n\", len(test_news), \"news:\", (t2-t1)/60, \"min\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iiFHUmSYaHsS"
      },
      "outputs": [],
      "source": [
        "t1 = time.time()\n",
        "valid_news['clean'] = valid_news['clean'].apply(spell_checker)\n",
        "valid_news.head(5)\n",
        "t2 = time.time()\n",
        "print(\"\\nTime to spell check the valid data: \\n\", len(valid_news), \"news:\", (t2-t1)/60, \"min\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Up12ZUnEaHsS"
      },
      "source": [
        "##### Saved the trained dataset into a seperate CSV file "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gwDzZHYuaHsS"
      },
      "outputs": [],
      "source": [
        "# train_news.to_csv(\"input_data/train_processed.csv\", sep=',')\n",
        "# test_news.to_csv(\"input_data/test_processed.csv\", sep=',')\n",
        "# valid_news.to_csv(\"input_data/valid_processed.csv\", sep=',')\n",
        "train_news.to_csv((file_path_data + '/train_processed.csv'), sep=',')\n",
        "test_news.to_csv((file_path_data + '/test_processed.csv'), sep=',')\n",
        "valid_news.to_csv((file_path_data + '/valid_processed.csv'), sep=',')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0My64JuaHsS"
      },
      "source": [
        "### Visualization \n",
        "\n",
        "#### WordCloud \n",
        "\n",
        "> As a tool for visualization by using the frequency of words appeared in text, we use WordCloud. Note that it can give more information and insight of texts by analyzing correlations and similarities between words rather than analyzing texts only by the frequency of words appeared; however, it can give you some general shape of what this text is about quickly and intuitively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cMZnVLwaHsS"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "YEjLx9CDaHsT"
      },
      "outputs": [],
      "source": [
        "def cloud(data,backgroundcolor = 'white', width = 800, height = 600):\n",
        "    wordcloud = WordCloud(stopwords = STOPWORDS, background_color = backgroundcolor,\n",
        "                         width = width, height = height).generate(data)\n",
        "    plt.figure(figsize = (15, 10))\n",
        "    plt.imshow(wordcloud)\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "    \n",
        "cloud(' '.join(train_news['clean']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_fbfjr7xaHsT"
      },
      "outputs": [],
      "source": [
        "cloud(' '.join(test_news['clean']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w0cbNEq4aHsT"
      },
      "outputs": [],
      "source": [
        "cloud(' '.join(valid_news['clean']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWyJWOMYaHsT"
      },
      "source": [
        "#### Inferences from visulaization: \n",
        "- The large words are the words that are frequently appeared in the text/corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature: Corpus Structure"
      ],
      "metadata": {
        "id": "cWrU7UVLyIlY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import necessary Libraries"
      ],
      "metadata": {
        "id": "P5XtNjkFYahN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn import metrics"
      ],
      "metadata": {
        "id": "OuLdM9ofyLXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Data Set"
      ],
      "metadata": {
        "id": "-ErE2JGIYd4L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "train_news = pd.read_csv((file_path_data + '/train.tsv'), sep='\\t', names = colnames, error_bad_lines=False)\n",
        "test_news = pd.read_csv((file_path_data + '/test.tsv'), sep='\\t', names = colnames, error_bad_lines=False)\n",
        "valid_news = pd.read_csv((file_path_data + '/valid.tsv'), sep='\\t', names = colnames, error_bad_lines=False)"
      ],
      "metadata": {
        "id": "mi4wY-QFVdss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## New Data"
      ],
      "metadata": {
        "id": "KsvwUatthj8p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Cleaning has been performed above\n",
        "trump_tweets"
      ],
      "metadata": {
        "id": "5DIrFPiVv9ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trump_tweets['date'] = pd.to_datetime(trump_tweets['date'])"
      ],
      "metadata": {
        "id": "8-KujNWy5y2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trump_tweets.sort_values(by='date')"
      ],
      "metadata": {
        "id": "URRrqtt-56Gr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trump_tweets['Binary Label'] = 1"
      ],
      "metadata": {
        "id": "rS5t1X7n7Zv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# News Scrapper"
      ],
      "metadata": {
        "id": "Vzi20A7pBT-z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request,sys,time\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "pagesToGet= 20\n",
        "\n",
        "upperframe=[]  \n",
        "for page in range(1,pagesToGet+1):\n",
        "    # print('processing page :', page)\n",
        "    url = 'https://www.politifact.com/factchecks/list/?page='+str(page)\n",
        "    # print(url)\n",
        "    \n",
        "    #an exception might be thrown, so the code should be in a try-except block\n",
        "    try:\n",
        "        #use the browser to get the url. This is suspicious command that might blow up.\n",
        "        page=requests.get(url)                             # this might throw an exception if something goes wrong.\n",
        "    \n",
        "    except Exception as e:                                   # this describes what to do if an exception is thrown\n",
        "        error_type, error_obj, error_info = sys.exc_info()      # get the exception information\n",
        "        print ('ERROR FOR LINK:',url)                          #print the link that cause the problem\n",
        "        print (error_type, 'Line:', error_info.tb_lineno)     #print error info and line that threw the exception\n",
        "        continue                                              #ignore this page. Abandon this and go back.\n",
        "    time.sleep(2)   \n",
        "    soup=BeautifulSoup(page.text,'html.parser')\n",
        "    frame=[]\n",
        "    links=soup.find_all('li',attrs={'class':'o-listicle__item'})\n",
        "    # print(len(links))\n",
        "    filename=\"NEWS.csv\"\n",
        "    f=open(filename,\"w\", encoding = 'utf-8')\n",
        "    headers=\"Statement,Link,Date, Source, Label\\n\"\n",
        "    f.write(headers)\n",
        "    \n",
        "    for j in links:\n",
        "        Statement = j.find(\"div\",attrs={'class':'m-statement__quote'}).text.strip()\n",
        "        Link = \"https://www.politifact.com\"\n",
        "        Link += j.find(\"div\",attrs={'class':'m-statement__quote'}).find('a')['href'].strip()\n",
        "        Date = j.find('div',attrs={'class':'m-statement__body'}).find('footer').text[-14:-1].strip()\n",
        "        Source = j.find('div', attrs={'class':'m-statement__meta'}).find('a').text.strip()\n",
        "        Label = j.find('div', attrs ={'class':'m-statement__content'}).find('img',attrs={'class':'c-image__original'}).get('alt').strip()\n",
        "        frame.append((Statement,Link,Date,Source,Label))\n",
        "        f.write(Statement.replace(\",\",\"^\")+\",\"+Link+\",\"+Date.replace(\",\",\"^\")+\",\"+Source.replace(\",\",\"^\")+\",\"+Label.replace(\",\",\"^\")+\"\\n\")\n",
        "    upperframe.extend(frame)\n",
        "f.close()\n",
        "politifact_df=pd.DataFrame(upperframe, columns=['Statement','Link','Date','Source','Label'])\n",
        "politifact_df.head()"
      ],
      "metadata": {
        "id": "53P8ktRhU188"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.unique(politifact_df['Label'])"
      ],
      "metadata": {
        "id": "mrbKdFTGYlXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Cleaning"
      ],
      "metadata": {
        "id": "rrvspPjbYhDx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_news['clean'] = train_news[\"headline_text\"].apply(cleaning) \n",
        "                \n",
        "test_news['clean'] = test_news[\"headline_text\"].apply(cleaning)\n",
        "\n",
        "valid_news['clean'] = valid_news[\"headline_text\"].apply(cleaning)\n",
        "\n",
        "trump_tweets['clean'] = trump_tweets[\"text\"].apply(cleaning)\n",
        "\n",
        "politifact_df['claen'] = politifact_df[\"Statement\"].apply(cleaning)"
      ],
      "metadata": {
        "id": "IQZTObpPYkj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_news.merge(trump_tweets)\n",
        "train_news.merge(politifact_df)"
      ],
      "metadata": {
        "id": "dngdnYZbZkbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CorpusStructure():\n",
        "    \n",
        "    def __init__(self):\n",
        "        \n",
        "        self.logistic_regression_pipeline = ''\n",
        "        self.random_forest_pipeline = ''\n",
        "        \n",
        "    \n",
        "    def trueFalseLabel(self, r):\n",
        "        '''\n",
        "        Consider Original --\tTrue, Mostly-tru, Half-true as True (1), \n",
        "        and Barely-true, False, Pants-fire, full-flop, half-flip as False (0)\n",
        "        '''\n",
        "\n",
        "        v = r['label']\n",
        "\n",
        "        if (v == 'true'):\n",
        "            return 1\n",
        "        if (v == 'mostly-true'):\n",
        "            return 1\n",
        "        if (v == 'half-true'):\n",
        "            return 1\n",
        "        if (v == 'barely-true'):\n",
        "            return 0\n",
        "        if (v == 'half-flip'):\n",
        "            return 0\n",
        "        if (v == 'full-flop'):\n",
        "            return 0\n",
        "        if (v == 'false'):\n",
        "            return 0\n",
        "        if (v == 'pants-fire'):\n",
        "            return 0\n",
        "    \n",
        "    def logistic_regression_pipline(self, ngram_rg=(1,1)):\n",
        "        '''\n",
        "        Use TFiDF and Count Vector \n",
        "        '''\n",
        "        tfidf_vectorizer = TfidfVectorizer(max_features=10000, ngram_range=ngram_rg)\n",
        "        count_vectorizer = CountVectorizer(ngram_range=ngram_rg)\n",
        "        \n",
        "        X = count_vectorizer.fit_transform(train_news['clean'])\n",
        "        # Testing the ngram generation:\n",
        "        print(\"Get first 20 feature names:\\n\")\n",
        "        print(count_vectorizer.get_feature_names()[: 20])\n",
        "        \n",
        "        X_train = tfidf_vectorizer.fit_transform(train_news['clean'].values)\n",
        "        X_test = tfidf_vectorizer.transform(test_news['clean'].values)\n",
        "        \n",
        "        train_news['Binary Label'] = train_news.apply(lambda row: self.trueFalseLabel(row), axis=1)\n",
        "        test_news['Binary Label'] = train_news.apply(lambda row: self.trueFalseLabel(row), axis=1)\n",
        "\n",
        "        y_train = train_news['Binary Label']\n",
        "        y_test = test_news['Binary Label']\n",
        "        \n",
        "        self.logistic_regression_pipeline = Pipeline([\n",
        "            ('vectorizer', tfidf_vectorizer),\n",
        "            ('clf', LogisticRegression(penalty='l2', C=98.684210526315795))\n",
        "            ])\n",
        "\n",
        "        # print(X_train.shape)\n",
        "        # print(y_train.shape)\n",
        "\n",
        "        self.logistic_regression_pipeline.fit(train_news['clean'].values, y_train)\n",
        "        predicts = self.logistic_regression_pipeline.predict(test_news['clean'].values)\n",
        "        score = metrics.accuracy_score(y_test, predicts)\n",
        "        print(\"Accuracy:   %0.5f\" % score)\n",
        "        \n",
        "        \n",
        "    def logistic_regression_predict(self, text):\n",
        "        predicted = self.logistic_regression_pipeline.predict([text])\n",
        "        predicedProb = self.logistic_regression_pipeline.predict_proba([text])[:,1]\n",
        "        return bool(predicted), float(predicedProb)\n",
        "    \n",
        "    def random_forest_pipline(self, ngram_rg=(2, 2)):\n",
        "                \n",
        "        tfidf_vectorizer = TfidfVectorizer(max_features=10000, ngram_range=ngram_rg)\n",
        "        count_vectorizer = CountVectorizer(ngram_range=ngram_rg)\n",
        "        \n",
        "        X = count_vectorizer.fit_transform(train_news['clean'])\n",
        "        \n",
        "        X_train = tfidf_vectorizer.fit_transform(train_news['clean'].values)\n",
        "        X_test = tfidf_vectorizer.transform(test_news['clean'].values)\n",
        "        \n",
        "        train_news['Binary Label'] = train_news.apply(lambda row: self.trueFalseLabel(row), axis=1)\n",
        "        test_news['Binary Label'] = train_news.apply(lambda row: self.trueFalseLabel(row), axis=1)\n",
        "\n",
        "        y_train = train_news['Binary Label']\n",
        "        y_test = test_news['Binary Label']\n",
        "        \n",
        "        self.random_forest_pipline = Pipeline([\n",
        "            ('vectorizer', tfidf_vectorizer),\n",
        "            ('clf', RandomForestClassifier(n_estimators = 300))\n",
        "            ])\n",
        "\n",
        "\n",
        "        self.random_forest_pipline.fit(train_news['clean'].values, y_train)\n",
        "        predicts = self.random_forest_pipline.predict(test_news['clean'].values)\n",
        "        score = metrics.accuracy_score(y_test, predicts)\n",
        "        # Model Accuracy, how often is the classifier correct?\n",
        "        print(\"Accuracy:\",metrics.accuracy_score(y_test, predicts))\n",
        "\n",
        "    def random_forest_predict(self, text):\n",
        "        predicted = self.random_forest_pipeline.predict([text])\n",
        "        predicedProb = self.random_forest_pipeline.predict_proba([text])[:,1]\n",
        "        return bool(predicted), float(predicedProb)\n",
        "    \n"
      ],
      "metadata": {
        "id": "_WpzLE1Qyr0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_structure = CorpusStructure()"
      ],
      "metadata": {
        "id": "K2ebCHnX4n5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "  corpus_structure.logistic_regression_pipline(ngram_rg=(i, i))\n",
        "  print(\"\\n================\\n\")"
      ],
      "metadata": {
        "id": "HUJqO4caJ_M0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_structure.random_forest_pipline(ngram_rg=(2, 2))"
      ],
      "metadata": {
        "id": "Bjah12BBKKBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reference\n",
        "\n",
        "1. https://alternusvera.wordpress.com/\n",
        "2. https://devopedia.org/text-corpus-for-nlp#summary\n",
        "3. https://www.corpusdata.org/\n",
        "4. [Twitter Sentiment Analysis of Trump’s Coronavirus Response](https://medium.com/swlh/coronavirus-python-tutorial-1-520cc960aac1)\n",
        "\n",
        "1.  https://medium.com/glose-team/how-to-evaluate-text-readability-with-nlp-9c04bd3f46a2\n",
        "2. https://scholar.smu.edu/cgi/viewcontent.cgi?article=1202&context=datasciencereview\n",
        "3. https://en.wikipedia.org/wiki/Readability\n",
        "4. Google Search: nlp reading level detection\n",
        "5. https://blog.insightdatascience.com/verbiage-using-nlp-to-improve-k-12-content-marketing-8906d2810fda\n",
        "6. https://www.geeksforgeeks.org/readability-index-pythonnlp/\n",
        "7. [Scraping Data Off Twitter Using Python | Twitterscraper + NLP + Data Visualization](https://www.youtube.com/watch?v=MpIi4HtCiVk&t=81s)\n",
        "8. [The Generative Style Transformer](https://medium.com/agara-labs/the-generative-style-transformer-3564bce04d04)\n",
        "9. [Two minutes NLP — Quick intro to Text Style Transfer](https://medium.com/nlplanet/two-minutes-nlp-quick-intro-to-text-style-transfer-61de9cbd4083)\n",
        "10. [Reinforcement Learning Based Text Style Transfer without Parallel Training Corpus](https://www.youtube.com/watch?v=r8Cn_0Nb-ng)\n",
        "11. [Scraping 1000’s of News Articles using 10 simple steps\n",
        "](https://towardsdatascience.com/scraping-1000s-of-news-articles-using-10-simple-steps-d57636a49755)\n",
        "12.\n",
        "\n",
        "# Dataset\n",
        "\n",
        "1. [Trump Twitter Archive](https://www.thetrumparchive.com/)"
      ],
      "metadata": {
        "id": "Z6vriQ3Kx4DD"
      }
    }
  ]
}